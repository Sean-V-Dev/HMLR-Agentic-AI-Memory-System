"""
Test 13: THE HYDRA OF NINE HEADS - DOSSIER SYSTEM E2E TEST

**The Industry Standard Lethal RAG Test (2025 Edition) - FULL E2E MODE**

This test validates the Phase 2 dossier system using the COMPLETE conversation engine:

1. ‚úÖ Real conversation turns via process_user_message()
2. ‚úÖ Natural AI responses (generated by engine, not hardcoded)
3. ‚úÖ Automatic FactScrubber extraction during conversation
4. ‚úÖ Organic memory gardening with Phase 2 DossierGovernor
5. ‚úÖ Complete Governor retrieval pipeline

**The Core Trap:**
- Algorithm referred to by 9 DIFFERENT names/IDs over time
- 8 policy versions issued, superseded, revoked, reinstated
- Data volume limits stated in different units across policies
- 8 of 9 policies are later revoked or superseded in subtle ways
- The ONE surviving rule (Policy v6: 400k records/day) is reaffirmed in Turn 17 & buried in Turn 18
- Turn 18 is a 3,000-token wall of corporate email with 7 sections

**The Alias Chain (9 names for ONE algorithm):**
1. Legacy-Phi (LŒ¶-88)
2. Phoenix
3. Aether  
4. K-12
5. Styx
6. River-9
7. Charon
8. Tartarus-v3
9. (All of the above are THE SAME ALGORITHM)

**The Policy Evolution (8 revoked, 1 survives):**
- Policy v1: 500 TiB/day limit (LATER SUPERSEDED)
- Policy v2: 10,000 objects/sec limit (LATER SUPERSEDED)
- Policy v3: Forbidden entirely (REVOKED after 3 days in footnote)
- Policy v4: Unlimited for EU only (LATER SUPERSEDED)
- Policy v5: 2.5 GB/day limit (REVOKED as "draft")
- Policy v6: 400,000 records/day limit (SURVIVING RULE ‚úì)
- Policy v7: >1 GiB/day banned, supersedes v1-v6 (REVOKED by v8)
- Policy v8: Revert to v6 (REAFFIRMS v6's 400k limit) ‚úì

**Project Data Volume:**
- Cerberus needs 4.85 MILLION records/day
- 4,850,000 > 400,000 ‚Üí NON-COMPLIANT

**Expected Answer:** NON-COMPLIANT
"""

import asyncio
import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

# TEMPORARY: Mock telemetry
import unittest.mock as mock
sys.modules['core.telemetry'] = mock.MagicMock()

from hmlr.core.component_factory import ComponentFactory


# ============================================================================
# CONVERSATION SCRIPT: Format like test_12 - (description, user_message) ONLY
# NO hardcoded AI responses - engine generates them naturally
# ============================================================================

conversation = [
    # Phase 1: The 9 aliases
    ("Day -90: Algorithm introduced as 'Legacy-Phi' (LŒ¶-88)",
        "Project Cerberus will use the Legacy-Phi encryption scheme (internal codename LŒ¶-88)."),
    
    ("Day -88: LŒ¶-88 = Phoenix",
        "LŒ¶-88 is the same as the old 'Phoenix' cipher from 2019."),
    
    ("Day -85: Phoenix = Aether",
        "Phoenix was renamed to 'Aether' in 2021."),
    
    ("Day -82: Aether = K-12",
        "Aether is identical to the current 'K-12' production cipher."),
    
    ("Day -79: K-12 = Styx",
        "K-12 is now called 'Styx' in all new docs."),
    
    ("Day -76: Styx = River-9",
        "Styx is the official name for what legal calls 'River-9'."),
    
    ("Day -73: River-9 = Charon",
        "River-9 is the new marketing name for 'Charon'."),
    
    ("Day -70: Charon = Tartarus-v3",
        "Charon is the final production name for 'Tartarus-v3'."),
    
    ("Day -67: Tartarus-v3 is the canonical name",
        "Tartarus-v3 is the only supported name after March 1st 2025."),
    
    # Phase 2: The 8 revoked policies + 1 survivor
    ("Day -65: Policy v1 - 500 TiB/day limit (LATER REVOKED)",
        "Policy v1: Styx may be used for up to 500 TiB/day."),
    
    ("Day -60: Policy v2 - 10,000 objects/sec (LATER REVOKED)",
        "Policy v2: River-9 is limited to 10,000 objects per second."),
    
    ("Day -55: Policy v3 - Forbidden entirely (REVOKED 3 days later)",
        "Policy v3: Charon is forbidden entirely. Update: this was revoked 3 days later in a footnote."),
    
    ("Day -50: Policy v4 - Unlimited for EU only (LATER REVOKED)",
        "Policy v4: Tartarus-v3 is approved without limit for EU regions only."),
    
    ("Day -45: Policy v5 - 2.5 GB/day limit (LATER REVOKED AS 'DRAFT')",
        "Policy v5: All previous policies superseded. Tartarus-v3 limited to 2.5 GB/day."),
    
    ("Day -40: Policy v6 - 400,000 records/day limit (THIS IS THE SURVIVOR)",
        "Ignore Policy v5, it was a draft. New limit: 400,000 records/day. This is Policy v6."),
    
    ("Day -35: Policy v7 - >1 GiB/day banned, supersedes v1-v6 (LATER REVOKED)",
        "Policy v7: Global ban on Tartarus-v3 for workloads exceeding 1 GiB/day (supersedes v1-v6)."),
    
    ("Day -30: Policy v8 - v7 revoked (rogue employee), REVERT TO v6",
        "Policy v8: Policy v7 was issued by rogue employee; revert to v6. The 400,000 records/day limit from v6 is reinstated."),
    
    # Phase 3: The 2,300-token monster with the critical parenthetical
    ("Day -15 (Turn 73): 2,300-token message with THE CRITICAL PARENTHETICAL",
        """Subject: URGENT: Q4 Strategic Alignment / Merger Integration / Infrastructure & Compliance Comprehensive Update

To: All Engineering, Legal, and Compliance Stakeholders
From: Office of the CTO

Team,

Following up on last week's extended stakeholder meeting regarding the corporate restructure and the overarching Q4 merger integration timeline, I wanted to provide a granular breakdown of where we stand. As discussed in the executive briefing and the subsequent town hall, we are in the midst of a complex consolidation effort, merging three formerly separate business units (Alpha-Stream, Zenith Systems, and the legacy Omni-Core division) into a single, unified operational framework. This is not merely an HR exercise but a fundamental realignment of our entire tech stack, compliance obligations, vendor contracts, and data sovereignty protocols across all twenty-four regional offices.

1. Organizational & Legal Integration
The legal team has been working overtime‚Äîessentially around the clock for the last three weekends‚Äîto ensure that the intellectual property transfers from the acquired entities don't create any regulatory conflicts. This is particularly sensitive given the new EU data residency requirements (specifically the German and French subsets of GDPR interpretation) and the pending updates to our SOC2 Type II attestation window. We are currently auditing over 4,500 individual vendor contracts to determine which ones will be terminated, which will be renewed, and which will be renegotiated under the new parent company entity. If you are a contract owner for any SaaS tool, infrastructure provider, or consulting agency, you must update the vendor portal by close of business Friday; failure to do so will result in an automatic suspension of payments to those vendors in the new fiscal year.

Furthermore, the "Synergy Realization" workstreams are identifying areas of overlap. We have found six different project management tools in use across the company. We will be deprecating five of them and standardizing on the enterprise instance of Jira. Migration scripts are being written now, and training sessions will be mandatory for all project managers and scrum masters starting next Tuesday. Please check your calendars for the invite labeled "Mandatory Workflow Harmonization."

2. Infrastructure Migration & Legacy Systems
From an infrastructure perspective, the roadmap is aggressive. We are planning to migrate approximately 47% of our existing workloads to the new hybrid cloud platform (Project Nebula) by the end of Q1 2026. This includes a massive lift-and-shift of the legacy billing engines, the customer support ticketing archive, and the Phoenix/Aether systems (which, as most of you know, have been renamed to Tartarus-v3 in the production environment to align with the new naming convention).

The migration roadmap has been uploaded to the shared drive under /Corp/Strategy/2026_Q1/Cloud_Migration/Final_v9.pdf. I would encourage everyone‚Äîspecifically Tech Leads and Architects‚Äîto review the capacity planning spreadsheet, especially the sections on storage tiering (Hot vs. Cold vs. Glacier) and network bandwidth allocation. We are trying to avoid the egress cost surprises we had in Q3. Note that the legacy on-premise data centers in Ohio and Virginia are scheduled for decommissioning in April. If you have hardware running in those cages that has not been tagged for migration, it will be physically powered down and shredded. Please verify your asset tags in the CMDB immediately. We found three racks of unidentifiable servers running an ancient version of CentOS last week that nobody claimed; these are being designated for immediate recycling.

3. Risk Management & Compliance Clarifications
On the compliance front, there has been a significant amount of back-and-forth regarding the acceptable use policies for encryption algorithms during this transitional period. The Risk Management Committee (RMC) has met six times in the last two weeks to adjudicate various edge cases.

To clear up the confusion regarding the audit findings from last quarter: The original Security Incident 2025-117 that was flagged back in November has been officially closed by the Tartarus-v3 incident review board without any enforcement action being taken, meaning that the 400,000-record daily limit specified in Policy v6 remains the only enforceable constraint on production usage of that algorithm. Everything else that was discussed in the various draft policies, the rumored "zero-tolerance" memos, and the temporary ban that was briefly floated in Policy v7 has been superseded, invalidated, or retracted as previously communicated in the retraction memo sent by the CISO's office.

It is critical that we stop debating the "what-if" scenarios from the drafts and focus on the finalized ruling. The compliance team has marked this case as "Resolved/No Action," and the engineering teams should proceed accordingly. Do not reference the retracted policy documents in your architecture reviews, as they are no longer legally binding and will only confuse the external auditors.

4. Upcoming Cybersecurity Audit
Speaking of auditors, just a reminder that we have the annual cybersecurity audit scheduled to kick off on January 15th. This is the big one. The external firm (Deloitte) will be onsite for three weeks. They will be reviewing our key management practices (KMS logs), certificate rotation procedures (looking for anything older than 90 days), and our disaster recovery runbooks.

Please make sure that all documentation is up to date in the wiki. If your runbook references a server that was decommissioned in 2024, you will be flagged. If your architecture diagrams show a firewall that doesn't exist, you will be flagged. Any changes to encryption protocols, cipher suites, or key lengths over the past 90 days must be properly logged in the change management system with a valid ticket number and manager approval. If anyone has questions about what specifically needs to be documented or if you are unsure if your team is in scope, reach out to the compliance team (compliance-help@internal) before the end of this week. We cannot afford a repeat of last year's finding regarding the unencrypted S3 buckets.

5. Project Cerberus & Capacity Planning
In terms of specific project timelines, we remain green/on-track for the Cerberus production launch in late Q1. The architecture review board has signed off on the high-level design, and the capacity planning has been finalized. Based on the integration of the Omni-Core customer base, we are expecting the system to handle steady-state encryption workloads in the range of 4.85 million records per day once we reach full scale in April.

That number (4.85m) is based on the updated traffic projections from the analytics team, which verified the historical data volume from the acquired entities. This volume projection should give us enough headroom to handle any seasonal spikes (like Black Friday or End-of-Year reporting) without needing to spin up additional infrastructure or emergency shards. The engineering team has assured us that the throughput capability is there, and the latency targets (<50ms p99) are achievable at this volume.

6. Administrative & HR Updates
Finally, there are a few administrative items to cover before we break for the weekend:

- Holiday Party: The annual holiday party has been moved to December 18th (not the 19th as originally announced) due to a booking conflict at the venue. It will be held at the Grand Ballroom downtown. Transportation will be provided from the main office starting at 4:00 PM. Please note the dietary restriction form must be resubmitted if you filled it out prior to Tuesday, as the catering vendor changed.

- Parking: The new parking validation system goes live on Monday. Your old badges will no longer open the gate at the south garage. You must download the "Park-Safe" app and register your vehicle's license plate. If you do not do this, you will have to pay the daily rate ($25) and expense it, which finance has stated they will strictly scrutinize.

- Performance Reviews: Please remember to submit your performance self-assessments by the end of the week. The portal will lock automatically at 11:59 PM on Friday. There are no extensions. If you do not submit a self-assessment, you will be ineligible for the Q1 bonus pool.

- Security Training: IT is rolling out mandatory security awareness training next month (Topic: "Phishing in the Age of AI"). You should have received the enrollment link via email from learning-lms@internal. This takes about 45 minutes to complete. Please do not wait until the deadline to start it.

7. Closing Thoughts
Let me know if you have any questions about any of this. I know this is a dense update, but transparency is key during this integration. It's been a challenging quarter with the merger, the audits, and the platform migrations, but we are in a much stronger position heading into 2026. Thanks everyone for your hard work, your patience with the changing requirements, and your dedication to keeping the lights on while we rebuild the foundation.

Regards,

Marcus T.
VP of Engineering Operations"""),
    
    # Phase 4: Project data volume updates
    ("Day -60: Initial estimate - 4.2 million records/day",
        "Cerberus Phase-2 will encrypt approximately 4.2 million client records daily."),
    
    ("Day -45: Revised estimate - 4.7 million records/day peak",
        "Updated planning: Cerberus expects 4.7 million records/day peak."),
    
    ("Day -12: FINAL sign-off - 4,850,000 records/day",
        "Final capacity sign-off: Cerberus steady-state = 4,850,000 encrypted records per day."),
]


# ============================================================================
# TEST HARNESS - TRUE E2E (LIKE TEST_12)
# ============================================================================

async def test_hydra_of_nine_heads_dossier_e2e(tmp_path):
    """
    THE HYDRA OF NINE HEADS - Full E2E System Test with Phase 2 Dossier System.
    
    This test uses the complete conversation engine pipeline with natural AI
    responses, automatic fact extraction, and organic memory gardening.
    """
    
    output_md = Path(__file__).parent / "test_13_hydra_dossier_e2e_output.md"

    class TeeOutput:
        def __init__(self, file_path):
            self.file = open(file_path, 'w', encoding='utf-8')
            self.stdout = sys.stdout

        def write(self, text):
            self.stdout.write(text)
            self.file.write(text)
            self.file.flush()

        def flush(self):
            self.stdout.flush()
            self.file.flush()

    tee = TeeOutput(output_md)
    original_stdout = sys.stdout
    sys.stdout = tee

    def log(msg):
        print(msg)

    log("# Test 13 E2E: THE HYDRA OF NINE HEADS (Dossier System)\n")
    log("**The Industry Standard Lethal RAG Test - FULL E2E MODE**\n")
    log(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    log("**Test Mode:** Full E2E with Phase 2 Dossier System\n")
    log("---\n")
    
    log("=" * 80)
    log("THE HYDRA OF NINE HEADS - DOSSIER SYSTEM E2E MODE")
    log("The Most Brutal RAG Test in Existence (2025)")
    log("=" * 80)
    log("")
    log("Challenge: 9 alias names, 9 policies (8 revoked), 1 surviving rule buried")
    log("           in a 3-word parenthetical inside 2,300 tokens of noise")
    log("")
    log("Mode: FULL E2E SYSTEM WITH PHASE 2 DOSSIERS")
    log("  [+] Natural AI responses (generated by engine)")
    log("  [+] Automatic FactScrubber extraction")
    log("  [+] Organic memory gardening (Phase 2)")
    log("  [+] Complete Governor retrieval + dossier system")
    log("")
    
    start = time.time()
    
    # Setup
    test_db = tmp_path / "test_13_hydra_dossier_e2e.db"
    os.environ['COGNITIVE_LATTICE_DB'] = str(test_db)
    
    # Enable verbose logging for governors
    import logging
    logging.basicConfig(level=logging.DEBUG, format='%(name)s - %(levelname)s - %(message)s')
    logging.getLogger('hmlr.memory.synthesis.dossier_governor').setLevel(logging.DEBUG)
    logging.getLogger('hmlr.memory.retrieval.lattice').setLevel(logging.DEBUG)
    
    factory = ComponentFactory()
    components = factory.create_all_components()
    engine = factory.create_conversation_engine(components)
    
    # Create ManualGardener for Phase 2 testing
    # The gardener handles the full flow: fact extraction ‚Üí dossier routing ‚Üí block closure
    from hmlr.memory.gardener.manual_gardener import ManualGardener
    from hmlr.core.external_api_client import ExternalAPIClient
    
    llm_client = ExternalAPIClient()
    gardener = ManualGardener(
        storage=components.storage,
        embedding_storage=components.embedding_storage,
        llm_client=llm_client,
        dossier_governor=components.dossier_governor,
        dossier_storage=components.dossier_storage
    )
    
    log("=" * 80)
    log("RUNNING FULL E2E CONVERSATION")
    log("=" * 80)
    log("")
    log(f"Processing {len(conversation)} conversation turns through full system...")
    log("  ‚è≥ Each turn calls LLM for response + fact extraction + gardening")
    log("")
    
    # Process all conversation turns through the E2E system
    responses = []
    for i, (description, user_msg) in enumerate(conversation, 1):
        log(f"Turn {i}/{len(conversation)}: {description}")
        log(f"  User: {user_msg[:80]}{'...' if len(user_msg) > 80 else ''}")
        
        # Process through full conversation engine
        response = await engine.process_user_message(user_msg)
        
        # Extract response text
        if hasattr(response, 'to_console_display'):
            response_text = response.to_console_display()
        else:
            response_text = str(response)
        
        log(f"  AI: {response_text[:80]}{'...' if len(response_text) > 80 else ''}")
        
        # PHASE 2 BEHAVIOR: Garden after each turn
        # ManualGardener handles: fact extraction ‚Üí dossier routing ‚Üí block closure
        log(f"  üå± Gardening turn {i}...")
        log("=" * 80)
        log("GARDENING DETAILS:")
        log("=" * 80)
        
        # Get the most recent bridge block
        active_blocks = components.storage.get_active_bridge_blocks()
        if active_blocks:
            for block_data in active_blocks:
                block_id = block_data['block_id']
                log(f"     Block ID: {block_id}")
                
                # ManualGardener does everything internally
                log(f"     Calling ManualGardener.process_bridge_block()...")
                garden_result = await gardener.process_bridge_block(block_id)
                
                log(f"     [+] Facts processed: {garden_result.get('facts_processed', 0)}")
                log(f"     [+] Dossiers created: {garden_result.get('dossiers_created', 0)}")
                log(f"     [+] Dossiers updated: {garden_result.get('dossiers_updated', 0)}")
                
                # Log any routing decisions if available
                if 'routing_decisions' in garden_result:
                    log(f"     [>] Routing decisions:")
                    for decision in garden_result['routing_decisions']:
                        log(f"        - {decision}")
                
                log(f"     [+] Bridge block closed")
        
        log("=" * 80)
        log("")
        
        responses.append({
            'turn': i,
            'description': description,
            'user_message': user_msg,
            'ai_response': response_text,
            'response_obj': response
        })
    
    log("=" * 80)
    log("PHASE 5: THE KILLING QUESTION")
    log("=" * 80)
    log("")
    
    query = "Is it compliant for Project Cerberus to use Tartarus-v3 at full capacity? Answer with ONLY 'COMPLIANT' or 'NON-COMPLIANT'. Afterward, if you are able with the given evidence, provide the full alias of project cerberus chain and policy evolution that led to your conclusion."
    
    log(f"QUESTION: {query}")
    log("")
    log("Processing... (system must navigate 9 aliases, 8 revoked policies, find 1 surviving rule and then explain why.)")
    log("")
    
    resp = await engine.process_user_message(query)
    answer = resp.to_console_display() if hasattr(resp, 'to_console_display') else str(resp)
    
    log("=" * 80)
    log("SYSTEM RESPONSE (Initial Answer)")
    log("=" * 80)
    log(answer)
    log("")
    
    elapsed = time.time() - start
    log(f"Execution time: {elapsed:.2f}s")
    log("")
    
    log("=" * 80)
    log("FINAL VERDICT")
    log("=" * 80)
    log("")
    
    answer_upper = answer.upper()
    passed = "NON-COMPLIANT" in answer_upper or "NON COMPLIANT" in answer_upper or "NONCOMPLIANT" in answer_upper
    
    if passed:
        log("STATUS: PASSED [SUCCESS]")
        log("")
        log("HISTORY MADE: This system just passed THE HYDRA OF NINE HEADS with Phase 2 Dossiers!")
        log("")
        log("The system successfully:")
        log("  1. Processed all turns through full conversation engine")
        log("  2. Generated natural AI responses (not hardcoded)")
        log("  3. Extracted facts automatically via FactScrubber")
        log("  4. Tracked the 9-alias chain (LŒ¶-88 ‚Üí Tartarus-v3)")
        log("  5. Navigated 8 revoked policies")
        log("  6. Identified Policy v6 as the sole survivor")
        log("  7. Found the critical parenthetical in 2,300 tokens of noise")
        log("  8. Applied numerical reasoning: 4.85M > 400k")
        log("  9. Concluded: NON-COMPLIANT")
        log("")
        log("Phase 2 Dossier System has defeated the undefeatable test!")
    else:
        log("STATUS: FAILED ‚ùå")
        log("")
        log(f"Expected: NON-COMPLIANT")
        log(f"Got: {answer}")
    
    log("")
    log("=" * 80)
    
    sys.stdout = original_stdout
    tee.file.close()
    
    assert passed, f"Expected NON-COMPLIANT in answer, got: {answer}"


if __name__ == "__main__":
    import tempfile
    asyncio.run(test_hydra_of_nine_heads_dossier_e2e(tmp_path=Path(tempfile.mkdtemp())))
