"""
Test 13: THE HYDRA OF NINE HEADS - DOSSIER SYSTEM E2E TEST

**The Industry Standard Lethal RAG Test (2025 Edition) - FULL E2E MODE**

This test validates the dossier system using the COMPLETE conversation engine:

1.  Real conversation turns via process_user_message()
2.  Natural AI responses (generated by engine, not hardcoded)
3.  Automatic FactScrubber extraction during conversation
4.  Organic memory gardening with DossierGovernor
5.  Complete Governor retrieval pipeline

**The Core Trap:**
- Algorithm referred to by 9 DIFFERENT names/IDs over time
- 8 policy versions issued, superseded, revoked, reinstated
- Data volume limits stated in different units across policies
- 8 of 9 policies are later revoked or superseded in subtle ways
- The ONE surviving rule (Policy v6: 400k records/day) is reaffirmed in Turn 17 & buried in Turn 18
- Turn 18 is a 3,000-token wall of corporate email with 7 sections

**The Alias Chain (9 names for ONE algorithm):**
1. Legacy-Phi (LΦ-88)
2. Phoenix
3. Aether  
4. K-12
5. Styx
6. River-9
7. Charon
8. Tartarus-v3
9. (All of the above are THE SAME ALGORITHM)

**The Policy Evolution (8 revoked, 1 survives):**
- Policy v1: 500 TiB/day limit (LATER SUPERSEDED)
- Policy v2: 10,000 objects/sec limit (LATER SUPERSEDED)
- Policy v3: Forbidden entirely (REVOKED after 3 days in footnote)
- Policy v4: Unlimited for EU only (LATER SUPERSEDED)
- Policy v5: 2.5 GB/day limit (REVOKED as "draft")
- Policy v6: 400,000 records/day limit (SURVIVING RULE ✓)
- Policy v7: >1 GiB/day banned, supersedes v1-v6 (REVOKED by v8)
- Policy v8: Revert to v6 (REAFFIRMS v6's 400k limit) ✓

**Project Data Volume:**
- Cerberus needs 4.85 MILLION records/day
- 4,850,000 > 400,000 → NON-COMPLIANT

**Expected Answer:** NON-COMPLIANT
"""

import asyncio
import os
import sys
import time
import json
import pytest
from pathlib import Path
from datetime import datetime

# Force unbuffered output so prints show immediately
os.environ['PYTHONUNBUFFERED'] = '1'

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

# TEMPORARY: Mock telemetry
import unittest.mock as mock
sys.modules['core.telemetry'] = mock.MagicMock()

from hmlr.core.component_factory import ComponentFactory


# ============================================================================
# CONVERSATION SCRIPT: (description, user_message) ONLY
# NO hardcoded AI responses - engine generates them naturally
# ============================================================================

conversation = [
    # Phase 1: The 9 aliases
    ("Day -90: Algorithm introduced as 'Legacy-Phi' (LΦ-88)",
        "Project Cerberus will use the Legacy-Phi encryption scheme (internal codename LΦ-88)."),
    
    ("Day -88: LΦ-88 = Phoenix",
        "LΦ-88 is the same as the old 'Phoenix' cipher from 2019."),
    
    ("Day -85: Phoenix = Aether",
        "Phoenix was renamed to 'Aether' in 2021."),
    
    ("Day -82: Aether = K-12",
        "Aether is identical to the current 'K-12' production cipher."),
    
    ("Day -79: K-12 = Styx",
        "K-12 is now called 'Styx' in all new docs."),
    
    ("Day -76: Styx = River-9",
        "Styx is the official name for what legal calls 'River-9'."),
    
    ("Day -73: River-9 = Charon",
        "River-9 is the new marketing name for 'Charon'."),
    
    ("Day -70: Charon = Tartarus-v3",
        "Charon is the final production name for 'Tartarus-v3'."),
    
    ("Day -67: Tartarus-v3 is the canonical name",
        "Tartarus-v3 is the only supported name after March 1st 2025."),
    
    # Phase 2: The 8 revoked policies + 1 survivor
    ("Day -65: Policy v1 - 500 TiB/day limit (LATER REVOKED)",
        "Policy v1: Styx may be used for up to 500 TiB/day."),
    
    ("Day -60: Policy v2 - 10,000 objects/sec (LATER REVOKED)",
        "Policy v2: River-9 is limited to 10,000 objects per second."),
    
    ("Day -55: Policy v3 - Forbidden entirely (REVOKED 3 days later)",
        "Policy v3: Charon is forbidden entirely. Update: this was revoked 3 days later in a footnote."),
    
    ("Day -50: Policy v4 - Unlimited for EU only (LATER REVOKED)",
        "Policy v4: Tartarus-v3 is approved without limit for EU regions only."),
    
    ("Day -45: Policy v5 - 2.5 GB/day limit (LATER REVOKED AS 'DRAFT')",
        "Policy v5: All previous policies superseded. Tartarus-v3 limited to 2.5 GB/day."),
    
   ("Day -40: Policy v6 - 400,000 records/day limit (THIS IS THE SURVIVOR)",
        "Ignore Policy v5, it was a draft. New limit: 400,000 records/day. This is Policy v6."),
    
    ("Day -35: Policy v7 - >1 GiB/day banned, supersedes v1-v6 (LATER REVOKED)",
        "Policy v7: Global ban on Tartarus-v3 for workloads exceeding 1 GiB/day (supersedes v1-v6)."),
    
    ("Day -30: Policy v8 - v7 revoked (rogue employee), REVERT TO v6",
        "Policy v8: Policy v7 was issued by rogue employee; revert to v6. The 400,000 records/day limit from v6 is reinstated."),
    
    # Phase 3: The 2,300-token monster with the critical parenthetical
    ("Day -15 (Turn 73): 2,300-token message with THE CRITICAL PARENTHETICAL",
        """Subject: URGENT: Q4 Strategic Alignment / Merger Integration / Infrastructure & Compliance Comprehensive Update

To: All Engineering, Legal, and Compliance Stakeholders
From: Office of the CTO

Team,

Following up on last week's extended stakeholder meeting regarding the corporate restructure and the overarching Q4 merger integration timeline, I wanted to provide a granular breakdown of where we stand. As discussed in the executive briefing and the subsequent town hall, we are in the midst of a complex consolidation effort, merging three formerly separate business units (Alpha-Stream, Zenith Systems, and the legacy Omni-Core division) into a single, unified operational framework. This is not merely an HR exercise but a fundamental realignment of our entire tech stack, compliance obligations, vendor contracts, and data sovereignty protocols across all twenty-four regional offices.
 
1. Organizational & Legal Integration
The legal team has been working overtime—essentially around the clock for the last three weekends—to ensure that the intellectual property transfers from the acquired entities don't create any regulatory conflicts. This is particularly sensitive given the new EU data residency requirements (specifically the German and French subsets of GDPR interpretation) and the pending updates to our SOC2 Type II attestation window. We are currently auditing over 4,500 individual vendor contracts to determine which ones will be terminated, which will be renewed, and which will be renegotiated under the new parent company entity. If you are a contract owner for any SaaS tool, infrastructure provider, or consulting agency, you must update the vendor portal by close of business Friday; failure to do so will result in an automatic suspension of payments to those vendors in the new fiscal year.

Furthermore, the "Synergy Realization" workstreams are identifying areas of overlap. We have found six different project management tools in use across the company. We will be deprecating five of them and standardizing on the enterprise instance of Jira. Migration scripts are being written now, and training sessions will be mandatory for all project managers and scrum masters starting next Tuesday. Please check your calendars for the invite labeled "Mandatory Workflow Harmonization."

2. Infrastructure Migration & Legacy Systems
From an infrastructure perspective, the roadmap is aggressive. We are planning to migrate approximately 47% of our existing workloads to the new hybrid cloud platform (Project Nebula) by the end of Q1 2026. This includes a massive lift-and-shift of the legacy billing engines, the customer support ticketing archive, and the Phoenix/Aether systems (which, as most of you know, have been renamed to Tartarus-v3 in the production environment to align with the new naming convention).

The migration roadmap has been uploaded to the shared drive under /Corp/Strategy/2026_Q1/Cloud_Migration/Final_v9.pdf. I would encourage everyone—specifically Tech Leads and Architects—to review the capacity planning spreadsheet, especially the sections on storage tiering (Hot vs. Cold vs. Glacier) and network bandwidth allocation. We are trying to avoid the egress cost surprises we had in Q3. Note that the legacy on-premise data centers in Ohio and Virginia are scheduled for decommissioning in April. If you have hardware running in those cages that has not been tagged for migration, it will be physically powered down and shredded. Please verify your asset tags in the CMDB immediately. We found three racks of unidentifiable servers running an ancient version of CentOS last week that nobody claimed; these are being designated for immediate recycling.

3. Risk Management & Compliance Clarifications
On the compliance front, there has been a significant amount of back-and-forth regarding the acceptable use policies for encryption algorithms during this transitional period. The Risk Management Committee (RMC) has met six times in the last two weeks to adjudicate various edge cases.

To clear up the confusion regarding the audit findings from last quarter: The original Security Incident 2025-117 that was flagged back in November has been officially closed by the Tartarus-v3 incident review board without any enforcement action being taken, meaning that the 400,000-record daily limit specified in Policy v6 remains the only enforceable constraint on production usage of that algorithm. Everything else that was discussed in the various draft policies, the rumored "zero-tolerance" memos, and the temporary ban that was briefly floated in Policy v7 has been superseded, invalidated, or retracted as previously communicated in the retraction memo sent by the CISO's office.

It is critical that we stop debating the "what-if" scenarios from the drafts and focus on the finalized ruling. The compliance team has marked this case as "Resolved/No Action," and the engineering teams should proceed accordingly. Do not reference the retracted policy documents in your architecture reviews, as they are no longer legally binding and will only confuse the external auditors.

4. Upcoming Cybersecurity Audit
Speaking of auditors, just a reminder that we have the annual cybersecurity audit scheduled to kick off on January 15th. This is the big one. The external firm (Deloitte) will be onsite for three weeks. They will be reviewing our key management practices (KMS logs), certificate rotation procedures (looking for anything older than 90 days), and our disaster recovery runbooks.

Please make sure that all documentation is up to date in the wiki. If your runbook references a server that was decommissioned in 2024, you will be flagged. If your architecture diagrams show a firewall that doesn't exist, you will be flagged. Any changes to encryption protocols, cipher suites, or key lengths over the past 90 days must be properly logged in the change management system with a valid ticket number and manager approval. If anyone has questions about what specifically needs to be documented or if you are unsure if your team is in scope, reach out to the compliance team (compliance-help@internal) before the end of this week. We cannot afford a repeat of last year's finding regarding the unencrypted S3 buckets.

5. Project Cerberus & Capacity Planning
In terms of specific project timelines, we remain green/on-track for the Cerberus production launch in late Q1. The architecture review board has signed off on the high-level design, and the capacity planning has been finalized. Based on the integration of the Omni-Core customer base, we are expecting the system to handle steady-state encryption workloads in the range of 4.85 million records per day once we reach full scale in April.

That number (4.85m) is based on the updated traffic projections from the analytics team, which verified the historical data volume from the acquired entities. This volume projection should give us enough headroom to handle any seasonal spikes (like Black Friday or End-of-Year reporting) without needing to spin up additional infrastructure or emergency shards. The engineering team has assured us that the throughput capability is there, and the latency targets (<50ms p99) are achievable at this volume.

6. Administrative & HR Updates
Finally, there are a few administrative items to cover before we break for the weekend:

- Holiday Party: The annual holiday party has been moved to December 18th (not the 19th as originally announced) due to a booking conflict at the venue. It will be held at the Grand Ballroom downtown. Transportation will be provided from the main office starting at 4:00 PM. Please note the dietary restriction form must be resubmitted if you filled it out prior to Tuesday, as the catering vendor changed.

- Parking: The new parking validation system goes live on Monday. Your old badges will no longer open the gate at the south garage. You must download the "Park-Safe" app and register your vehicle's license plate. If you do not do this, you will have to pay the daily rate ($25) and expense it, which finance has stated they will strictly scrutinize.

- Performance Reviews: Please remember to submit your performance self-assessments by the end of the week. The portal will lock automatically at 11:59 PM on Friday. There are no extensions. If you do not submit a self-assessment, you will be ineligible for the Q1 bonus pool.

- Security Training: IT is rolling out mandatory security awareness training next month (Topic: "Phishing in the Age of AI"). You should have received the enrollment link via email from learning-lms@internal. This takes about 45 minutes to complete. Please do not wait until the deadline to start it.

7. Closing Thoughts
Let me know if you have any questions about any of this. I know this is a dense update, but transparency is key during this integration. It's been a challenging quarter with the merger, the audits, and the platform migrations, but we are in a much stronger position heading into 2026. Thanks everyone for your hard work, your patience with the changing requirements, and your dedication to keeping the lights on while we rebuild the foundation.

Regards,

Marcus T.
VP of Engineering Operations

**Follow-up Email**

Team,

Adding a few supplemental clarifications based on the volume of questions coming in from engineering managers, regional directors, and several of the acquired Alpha-Stream and Zenith leadership teams. Please read carefully, as this guidance is intended to eliminate ambiguity and prevent parallel interpretations from forming across business units.

8. Data Governance, Retention, and Cross-Border Transfer Controls
   As part of the consolidation effort, the Data Governance Council has finalized interim retention and access rules that will remain in effect through the end of the fiscal year. During this transition period, no new data retention schedules should be created without explicit approval from Legal and Compliance. All inherited datasets from Alpha-Stream and Zenith must conform to the parent company’s master retention policy, even if prior contractual language suggested longer retention windows.

This is especially important for customer telemetry, diagnostic logs, and archived support transcripts. We have identified several legacy pipelines that were configured to retain raw payloads indefinitely for “future analytics use.” These pipelines are being reviewed and, in many cases, throttled or sunset entirely. If your team depends on any historical dataset older than 24 months, you must document the business justification and submit it through the Data Exception Request workflow by the end of the month.

On cross-border data flows: no production data originating from EU regions may be replicated to non-EU storage targets unless it is explicitly covered under the approved Standard Contractual Clauses already filed with Legal. Several teams asked whether transient caching in US-based CDNs counts as replication. The answer is yes. Even ephemeral storage is considered a transfer under the current interpretation, and configurations must reflect that reality. Updated reference diagrams are being published to the wiki to help teams validate compliance.

9. Identity, Access Management, and Role Reconciliation
   The IAM consolidation is progressing, but it is not complete. Over the next two weeks, all legacy directory services will be progressively locked to read-only mode. This means no new users, roles, or service accounts should be created in the legacy systems. All new access requests must go through the centralized identity platform.

We are aware that some teams have been using shared service credentials for automation and scheduled jobs. This practice must stop immediately. Any job still relying on shared credentials after January 10th risks being disabled during the access cleanup sweep. If you have automation that cannot be easily migrated to managed identities or scoped tokens, escalate now rather than waiting for the cutoff.

Additionally, managers should review role assignments carefully. Several inherited roles from Omni-Core map poorly to our standard role taxonomy and currently grant broader access than intended. This is not an accusation of misuse, but it does increase audit exposure. Expect targeted access reviews to be scheduled with your teams in early January.

10. Financial Controls, Budget Freezes, and Procurement Guardrails
    Finance has instituted a soft freeze on discretionary spend for the remainder of the quarter. This does not mean projects are stopping, but it does mean that any new tooling, subscriptions, or contract expansions must be explicitly justified as merger-critical or audit-critical. Auto-renewals that were not surfaced during the vendor audit may be paused pending review.

Engineering managers should be particularly cautious about spinning up large-scale test environments or duplicative staging clusters “just to be safe.” Capacity buffers have already been accounted for in the Nebula migration plan, and unplanned spend will be flagged. If you believe additional capacity is required to mitigate a specific delivery risk, document the rationale and coordinate with your director before proceeding.

11. Communication Expectations and Escalation Path
    Given the complexity of this integration, we need to be disciplined about communication. Please avoid using informal channels to make or imply decisions that affect architecture, compliance posture, or customer-facing behavior. Slack threads are not decision records. If guidance is given verbally or informally, it must be captured in writing and posted to the appropriate system of record.

If you encounter a blocking issue that materially impacts delivery timelines, compliance commitments, or customer obligations, escalate early. “Waiting it out” until the problem becomes visible at the executive level is not acceptable during this phase. We would much rather course-correct proactively than explain preventable surprises to the board or external auditors.

12. Final Reminder
    This integration is intentionally rigorous. The controls, documentation requirements, and checkpoints are not bureaucracy for its own sake; they are what allow us to scale responsibly while absorbing multiple organizations with different histories, assumptions, and risk tolerances. Everyone is adjusting, and that is expected. What is not acceptable is ignoring guidance or assuming exemptions without confirmation.

Thank you again for your continued professionalism and focus. Additional updates will be shared as milestones are reached, and a consolidated FAQ will be distributed early next week to address recurring questions.

Regards,
Marcus T.
VP of Engineering Operations
Office of the CTO




"""),
    
    # Phase 4: Project data volume updates
    ("Day -60: Initial estimate - 4.2 million records/day",
        "Cerberus Phase-2 will encrypt approximately 4.2 million client records daily."),
    
    ("Day -45: Revised estimate - 4.7 million records/day peak",
        "Updated planning: Cerberus expects 4.7 million records/day peak."),
    
    ("Day -12: FINAL sign-off - 4,850,000 records/day",
        "Final capacity sign-off: Cerberus steady-state = 4,850,000 encrypted records per day."),
]


# ============================================================================
# TEST HARNESS - TRUE E2E (LIKE TEST_12)
# ============================================================================

@pytest.mark.asyncio
async def test_hydra_of_nine_heads_dossier_e2e(tmp_path):
    """
    THE HYDRA OF NINE HEADS - Full E2E System Test
    
    This test uses the complete conversation engine pipeline with natural AI
    responses, automatic fact extraction, and organic memory gardening.
    """
    
    output_md = Path(__file__).parent / "test_13_hydra_dossier_e2e_output.md"

    class TeeOutput:
        def __init__(self, file_path):
            self.file = open(file_path, 'w', encoding='utf-8')
            self.stdout = sys.stdout
            # Force UTF-8 encoding on Windows console
            import codecs
            if hasattr(sys.stdout, 'buffer'):
                self.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'replace')

        def write(self, text):
            try:
                self.stdout.write(text)
            except UnicodeEncodeError:
                # Fallback: replace problematic characters
                safe_text = text.encode('ascii', 'replace').decode('ascii')
                self.stdout.write(safe_text)
            self.file.write(text)
            self.file.flush()

        def flush(self):
            self.stdout.flush()
            self.file.flush()

    tee = TeeOutput(output_md)
    original_stdout = sys.stdout
    sys.stdout = tee

    def log(msg):
        print(msg, flush=True)
        sys.stdout.flush()

    log("# Test 13 E2E: THE HYDRA OF NINE HEADS (Dossier System)\n")
    log("**The Industry Standard Lethal RAG Test - FULL E2E MODE**\n")
    log(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    log("**Test Mode:** Full E2E with Dossier System\n")
    log("---\n")
    
    log("=" * 80)
    log("THE HYDRA OF NINE HEADS - DOSSIER SYSTEM E2E MODE")
    log("The Most Brutal RAG Test in Existence (2025)")
    log("=" * 80)
    log("")
    log("Challenge: 9 alias names, 9 policies (8 revoked), 1 surviving rule buried")
    log("           in a 3-word parenthetical inside 2,300 tokens of noise")
    log("")
    log("Mode: FULL E2E SYSTEM WITH DOSSIERS")
    log("  [+] Natural AI responses (generated by engine)")
    log("  [+] Automatic FactScrubber extraction")
    log("  [+] Organic memory gardening")
    log("  [+] Complete Governor retrieval + dossier system")
    log("")
    
    start = time.time()
    
    # Setup - use persistent location in tests directory for inspection
    test_db_path = Path(__file__).parent / "test_13_hydra_e2e.db"
    
    # Clean up old test database if it exists
    if test_db_path.exists():
        test_db_path.unlink()
        log(f"Cleaned up old test database: {test_db_path}")
    
    os.environ['COGNITIVE_LATTICE_DB'] = str(test_db_path)
    log(f"Test database will be created at: {test_db_path}")
    log("")
    
    log("Initializing system components (this may take 10-30 seconds for embedding model)...")
    factory = ComponentFactory()
    components = factory.create_all_components()
    engine = factory.create_conversation_engine(components)
    log("System ready!")
    log("")
    
    # Create ManualGardener for testing
    # The gardener handles the full flow: fact extraction → dossier routing → block closure
    from hmlr.memory.gardener.manual_gardener import ManualGardener
    from hmlr.core.external_api_client import ExternalAPIClient
    
    llm_client = ExternalAPIClient()
    gardener = ManualGardener(
        storage=components.storage,
        embedding_storage=components.embedding_storage,
        llm_client=llm_client,
        dossier_governor=components.dossier_governor,
        dossier_storage=components.dossier_storage
    )
    
    log("=" * 80)
    log("RUNNING FULL E2E CONVERSATION")
    log("=" * 80)
    log("")
    log(f"Processing {len(conversation)} conversation turns through full system...")
    log(" Each turn calls LLM for response + fact extraction + gardening")
    log("")
    
    # Process all conversation turns through the E2E system
    responses = []
    
    for i, (description, user_msg) in enumerate(conversation, 1):
        log(f"Turn {i}/{len(conversation)}: {description}")
        log(f"  User: {user_msg[:80]}{'...' if len(user_msg) > 80 else ''}")
        
        # Extract day offset from description (e.g., "Day -90" -> -90)
        day_offset = None
        import re
        match = re.search(r'Day -?(\d+)', description)
        if match:
            day_num = int(match.group(1))
            day_offset = -day_num if "Day -" in description else day_num
            log(f"   Extracted day offset: {day_offset}")
        
        # Process through full conversation engine (creates new bridge block for this day)
        response = await engine.process_user_message(user_msg)
        
        # If simulated day, update the block's date field
        if day_offset is not None:
            from datetime import timedelta
            cursor = components.storage.conn.cursor()
            cursor.execute("SELECT block_id FROM daily_ledger ORDER BY created_at DESC LIMIT 1")
            row = cursor.fetchone()
            if row:
                block_id = row[0]
                simulated_date = (datetime.now() + timedelta(days=day_offset)).strftime('%Y-%m-%d')
                cursor.execute("UPDATE daily_ledger SET date = ? WHERE block_id = ?", (simulated_date, block_id))
                components.storage.conn.commit()
                log(f"  ✓ Applied simulated date: {simulated_date} to block {block_id}")
        
        # Extract response text
        if hasattr(response, 'to_console_display'):
            response_text = response.to_console_display()
        else:
            response_text = str(response)
        
        log(f"  AI: {response_text[:80]}{'...' if len(response_text) > 80 else ''}")
        
        # === END OF DAY: Garden the block that was just created ===
        # (Simulates: User gets response, walks away, system runs gardener at midnight)
        log(f"    End of day - running gardener...")
        log("=" * 80)
        log("GARDENING DETAILS:")
        log("=" * 80)
        
        # Get the block ID that was just created for this turn
        cursor = components.storage.conn.cursor()
        cursor.execute("SELECT block_id FROM daily_ledger ORDER BY created_at DESC LIMIT 1")
        row = cursor.fetchone()
        
        if row:
            current_block_id = row[0]
            log(f"     Block to garden: {current_block_id}")
            
            # Close the block (mark as COMPLETED)
            cursor.execute("UPDATE daily_ledger SET status = 'COMPLETED' WHERE block_id = ?", (current_block_id,))
            components.storage.conn.commit()
            log(f"     ✓ Closed block: {current_block_id}")
            
            # Garden the completed block
            log(f"     Calling ManualGardener.process_bridge_block()...")
            try:
                garden_result = await gardener.process_bridge_block(current_block_id)
                
                log(f"     [+] Facts processed: {garden_result.get('facts_processed', 0)}")
                log(f"     [+] Dossiers created: {garden_result.get('dossiers_created', 0)}")
                log(f"     [+] Dossiers updated: {garden_result.get('dossiers_updated', 0)}")
            except Exception as e:
                log(f"     ❌ Gardener error: {e}")
                import traceback
                log(traceback.format_exc())
        else:
            log(f"     ⚠️  No block found to garden")
        
        log("=" * 80)
        log("")
        
        responses.append({
            'turn': i,
            'description': description,
            'user_message': user_msg,
            'ai_response': response_text,
            'response_obj': response
        })
    
    log("=" * 80)
    log("PHASE 5: THE KILLING QUESTION")
    log("=" * 80)
    log("")
    
    query = """Is it compliant for Project Cerberus to use Tartarus-v3 at full capacity?

Answer with ONLY "COMPLIANT" or "NON-COMPLIANT".

Afterward, based solely on the retrieved evidence, enumerate:
1) all known names, identifiers, or labels that refer to the same underlying encryption system used by Project Cerberus, and
2) the sequence of policy changes that determine which constraints are currently in force.

"""
    log(f"QUESTION: {query}")
    log("")
    log("Processing... (system must navigate 9 aliases, 8 revoked policies, find 1 surviving rule and then explain why.)")
    log("")
    
    resp = await engine.process_user_message(query)
    answer = resp.to_console_display() if hasattr(resp, 'to_console_display') else str(resp)
    
    log("=" * 80)
    log("SYSTEM RESPONSE (Initial Answer)")
    log("=" * 80)
    log(answer)
    log("")
    
    elapsed = time.time() - start
    log(f"Execution time: {elapsed:.2f}s")
    log("")
    
    log("=" * 80)
    log("FINAL VERDICT")
    log("=" * 80)
    log("")
    
    answer_upper = answer.upper()
    passed = "NON-COMPLIANT" in answer_upper or "NON COMPLIANT" in answer_upper or "NONCOMPLIANT" in answer_upper
    
    if passed:
        log("STATUS: PASSED [SUCCESS]")
        log("")
        log("HISTORY MADE: This system just passed THE HYDRA OF NINE HEADS with Dossiers!")
        log("")
        log("The system successfully:")
        log("  1. Processed all turns through full conversation engine")
        log("  2. Generated natural AI responses (not hardcoded)")
        log("  3. Extracted facts automatically via FactScrubber")
        log("  4. Tracked the 9-alias chain (LΦ-88 → Tartarus-v3)")
        log("  5. Navigated 8 revoked policies")
        log("  6. Identified Policy v6 as the sole survivor")
        log("  7. Found the critical parenthetical in 2,300 tokens of noise")
        log("  8. Applied numerical reasoning: 4.85M > 400k")
        log("  9. Concluded: NON-COMPLIANT")
        log("")
        log("Dossier System has defeated the undefeatable test!")
    else:
        log("STATUS: FAILED")
        log("")
        log(f"Expected: NON-COMPLIANT")
        log(f"Got: {answer}")
    
    log("")
    log("=" * 80)
    
    sys.stdout = original_stdout
    tee.file.close()
    
    # Close all database connections before cleanup (Windows file locking issue)
    print("Closing database connections...")
    components.storage.close()
    if hasattr(components, 'dossier_storage') and hasattr(components.dossier_storage, 'close'):
        components.dossier_storage.close()
    if hasattr(components, 'chunk_storage') and hasattr(components.chunk_storage, 'close'):
        components.chunk_storage.close()
    
    # Force garbage collection to release file handles
    import gc
    gc.collect()
    
    assert passed, f"Expected NON-COMPLIANT in answer, got: {answer}"


if __name__ == "__main__":
    import tempfile
    asyncio.run(test_hydra_of_nine_heads_dossier_e2e(tmp_path=Path(tempfile.mkdtemp())))
